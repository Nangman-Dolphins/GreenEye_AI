# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RlL8pAima9BG5bvjhSmHCHsJLQ2LH8IZ
"""

!pip install pytorch-lightning
!pip install -U torchvision

import os
import argparse
import torch
import pytorch_lightning as pl
from torch import nn
from torch.nn import functional as F
from torch.utils.data import DataLoader, default_collate
from torchvision.datasets import ImageFolder
from torchvision import __version__ as torchvision_version
from torchvision.transforms import v2 as transforms
from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint

from google.colab import drive
drive.mount('/content/drive')

import math
from torch.optim.lr_scheduler import LambdaLR
from torchmetrics.classification import MulticlassF1Score, MulticlassPrecision, MulticlassRecall
import torchvision.models as models
import torch.nn as nn # Import torch.nn

try:
    # torchvision v0.17+ (or where SoftTargetCrossEntropyLoss exists)
    from torchvision.v2.loss import CrossEntropyLoss as SoftTargetCrossEntropyLoss
except ImportError:
    # 이전 버전 호환 또는 기본 PyTorch CrossEntropyLoss
    # SoftTargetCrossEntropyLoss가 torchvision에 없는 경우 torch.nn.CrossEntropyLoss 사용
    from torch.nn import CrossEntropyLoss as SoftTargetCrossEntropyLoss


def get_cosine_schedule_with_warmup(optimizer, warmup_epochs, total_epochs):
    def lr_lambda(current_epoch):
        if current_epoch < warmup_epochs:
            return float(current_epoch) / float(max(1, warmup_epochs))
        progress = float(current_epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))
        return 0.5 * (1.0 + math.cos(math.pi * progress))
    return LambdaLR(optimizer, lr_lambda)

class ProductionPlantClassifier(pl.LightningModule):
    """
    모든 피드백이 반영된 최종 버전의 분류 모델입니다.
    """
    def __init__(
        self,
        num_classes,
        total_epochs,
        learning_rate=1e-4,
        unfreeze_start_epoch=5,
        warmup_epochs=3,
        dropout_rate=0.2,
        weight_decay=1e-5,
    ):
        super().__init__()
        self.save_hyperparameters()
        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
        for name, param in self.resnet.named_parameters():
            if not name.startswith(("layer4", "fc")):
                param.requires_grad = False
        in_features = self.resnet.fc.in_features
        self.resnet.fc = nn.Sequential(nn.Dropout(p=self.hparams.dropout_rate), nn.Linear(in_features, num_classes))
        # Use the imported SoftTargetCrossEntropyLoss (which will be torch.nn.CrossEntropyLoss if torchvision.v2 import fails)
        self.soft_loss_fn = SoftTargetCrossEntropyLoss()
        self.metrics = nn.ModuleDict({
            'f1': MulticlassF1Score(num_classes=num_classes, average='macro'),
            'precision': MulticlassPrecision(num_classes=num_classes, average='macro'),
            'recall': MulticlassRecall(num_classes=num_classes, average='macro')
        })

    def _get_loss(self, y_hat, y):
        # 라벨 타입에 따라 다른 손실 함수 적용
        if y.ndim > 1 and y.shape[1] == self.hparams.num_classes:
            # Check if SoftTargetCrossEntropyLoss is available, otherwise use standard CrossEntropyLoss
            if isinstance(self.soft_loss_fn, nn.CrossEntropyLoss):
                 # For standard CrossEntropyLoss with soft labels, use F.cross_entropy
                 return F.cross_entropy(y_hat, y)
            else:
                # Use SoftTargetCrossEntropyLoss if successfully imported from torchvision.v2
                return self.soft_loss_fn(y_hat, y) # for CutMix/MixUp soft labels
        else:
            return F.cross_entropy(y_hat, y) # for standard hard labels

    def on_train_epoch_start(self):
        # 특정 에포크에 백본 동결 해제 및 옵티마이저 상태 초기화
        if self.current_epoch == self.hparams.unfreeze_start_epoch:
            print(f"\n[Epoch {self.current_epoch}] Unfreezing backbone and resetting optimizer state.")
            for p in self.resnet.parameters():
                if not p.requires_grad:
                    p.requires_grad = True
            opt = self.trainer.optimizers[0]
            fc_ids = {id(p) for p in self.resnet.fc.parameters()}
            for p, state in list(opt.state.items()):
                if id(p) not in fc_ids:
                    opt.state.pop(p)

    def forward(self, x):
        return self.resnet(x)

    def _calculate_and_log_metrics(self, step_name, y_hat, y, on_step=False, on_epoch=True, prog_bar=False):
        preds = y_hat.argmax(dim=1)
        # Ensure targets are correctly formatted (long type for CrossEntropyLoss and torchmetrics)
        targets = y.argmax(dim=1).long() if y.ndim > 1 else y.long()
        acc = (preds == targets).float().mean()
        self.log(f"{step_name}_acc", acc, on_step=on_step, on_epoch=on_epoch, prog_bar=True)
        metric_results = {name: metric(preds, targets) for name, metric in self.metrics.items()}
        self.log_dict({f"{step_name}_{k}": v for k, v in metric_results.items()}, on_step=on_step, on_epoch=on_epoch, prog_bar=prog_bar)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self._get_loss(y_hat, y)
        self.log("train_loss", loss, on_step=True, on_epoch=True)
        self._calculate_and_log_metrics("train", y_hat, y, on_step=True, on_epoch=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self._get_loss(y_hat, y)
        self.log("val_loss", loss, on_step=False, on_epoch=True, prog_bar=True)
        self._calculate_and_log_metrics("val", y_hat, y, on_step=False, on_epoch=True, prog_bar=True)

    def test_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self._get_loss(y_hat, y)
        self.log("test_loss", loss, on_step=False, on_epoch=True)
        self._calculate_and_log_metrics("test", y_hat, y, on_step=False, on_epoch=True)

    def configure_optimizers(self):
        # BN, Bias 파라미터는 weight decay에서 제외
        no_decay = []
        decay = []
        for name, param in self.named_parameters():
            if not param.requires_grad: continue
            if param.ndim <= 1 or name.endswith(".bias"):
                no_decay.append(param)
            else:
                decay.append(param)

        optimizer_groups = [
            {'params': decay, 'weight_decay': self.hparams.weight_decay},
            {'params': no_decay, 'weight_decay': 0.0}
        ]

        optimizer = torch.optim.AdamW(optimizer_groups, lr=self.hparams.learning_rate)

        scheduler = get_cosine_schedule_with_warmup(
            optimizer,
            warmup_epochs=self.hparams.warmup_epochs,
            total_epochs=self.hparams.total_epochs
        )
        return {"optimizer": optimizer, "lr_scheduler": {"scheduler": scheduler, "interval": "epoch"}}

class PlantDataModule(pl.LightningDataModule):
    def __init__(self, data_dir: str, batch_size: int, image_size: int, use_cutmix: bool, use_mixup: bool, num_workers: int):
        super().__init__()
        self.save_hyperparameters()
        self.mean, self.std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]
        self.num_classes_val = None
        self.collator = None

    def setup(self, stage: str = None):
        if self.num_classes_val is None:
            train_path = os.path.join(self.hparams.data_dir, "train")
            self.num_classes_val = len([d for d in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, d))])

        aug_list = []
        if self.hparams.use_cutmix and self.hparams.use_mixup:
            raise ValueError("CutMix와 MixUp을 동시에 사용할 수 없습니다.")
        if self.hparams.use_cutmix: aug_list.append(transforms.CutMix(num_classes=self.num_classes_val, alpha=1.0))
        if self.hparams.use_mixup: aug_list.append(transforms.MixUp(num_classes=self.num_classes_val, alpha=1.0))
        self.collator = aug_list[0] if aug_list else None

        self.train_transform = transforms.Compose([
            transforms.RandomResizedCrop(self.hparams.image_size, scale=(0.8, 1.0)),
            transforms.RandomHorizontalFlip(),
            transforms.TrivialAugmentWide(),
            transforms.ToTensor(),
            transforms.Normalize(mean=self.mean, std=self.std),
        ])

        self.val_test_transform = transforms.Compose([
            transforms.Resize(self.hparams.image_size + 32),
            transforms.CenterCrop(self.hparams.image_size),
            transforms.ToTensor(),
            transforms.Normalize(mean=self.mean, std=self.std),
        ])

        if stage == "fit" or stage is None:
            self.train_dataset = ImageFolder(root=os.path.join(self.hparams.data_dir, "train"), transform=self.train_transform)
            self.val_dataset = ImageFolder(root=os.path.join(self.hparams.data_dir, "val"), transform=self.val_test_transform)
        if stage == "test" or stage is None:
            self.test_dataset = ImageFolder(root=os.path.join(self.hparams.data_dir, "test"), transform=self.val_test_transform)

    def _collate_fn(self, batch):
        imgs, labels = default_collate(batch)
        assert imgs.ndim == 4, f"이미지 텐서는 4차원이어야 합니다. 현재: {imgs.ndim}D"
        if self.collator:
            return self.collator(imgs, labels)
        return imgs, labels

    def train_dataloader(self):
        return DataLoader(
            self.train_dataset, batch_size=self.hparams.batch_size, shuffle=True,
            num_workers=self.hparams.num_workers, pin_memory=True,
            persistent_workers=self.hparams.num_workers > 0, collate_fn=self._collate_fn
        )

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,
                          persistent_workers=self.hparams.num_workers > 0, pin_memory=True)

    def test_dataloader(self):
        return DataLoader(self.test_dataset, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,
                          persistent_workers=self.hparams.num_workers > 0, pin_memory=True)

def main(args):
    assert float(torchvision_version.split('.')[1]) >= 17, "이 스크립트는 TorchVision 0.17.0 이상이 필요합니다."
    torch.backends.cudnn.benchmark = True
    pl.seed_everything(42, workers=True)

    datamodule = PlantDataModule(
        data_dir=args.data_dir,
        batch_size=args.batch_size,
        image_size=args.image_size,
        use_cutmix=args.use_cutmix,
        use_mixup=args.use_mixup,
        num_workers=args.num_workers
    )

    datamodule.setup("fit")

    model = ProductionPlantClassifier(
        num_classes=datamodule.num_classes_val,
        total_epochs=args.epochs,
        learning_rate=args.lr,
        unfreeze_start_epoch=args.unfreeze_epoch,
        warmup_epochs=args.warmup_epochs,
        weight_decay=args.weight_decay
    )

    lr_monitor = LearningRateMonitor(logging_interval='epoch')
    checkpoint_callback = ModelCheckpoint(
        monitor='val_f1', mode='max',
        dirpath=args.checkpoint_dir,
        filename='plant-model-{epoch:02d}-{val_f1:.3f}',
        save_top_k=1, verbose=True
    )

    trainer = pl.Trainer(
        max_epochs=args.epochs,
        accelerator='gpu', devices=args.gpus,
        precision="16-mixed",
        gradient_clip_val=1.0,
        accumulate_grad_batches=args.accumulate_grad_batches,
        callbacks=[lr_monitor, checkpoint_callback],
        log_every_n_steps=10
    )

    print("🚀 학습을 시작합니다...")
    trainer.fit(model, datamodule=datamodule)

    print("✅ 학습이 완료되었습니다. 최적의 모델로 테스트를 진행합니다...")
    trainer.test(datamodule=datamodule, ckpt_path='best')

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="PyTorch Lightning Plant Disease Classifier Training")

    # 경로 및 기본 설정
    parser.add_argument("--data_dir", type=str, default= r"F:\images", help="데이터셋 경로")
    parser.add_argument("--checkpoint_dir", type=str, default=r"F:\checkpoints", help="체크포인트 저장 경로")
    parser.add_argument("--gpus", type=int, default=1, help="사용할 GPU 수")
    # num_workers의 기본값을 0으로 변경하여 멀티프로세싱 관련 문제를 방지합니다.
    parser.add_argument("--num_workers", type=int, default=0, help="데이터 로더 워커 수")

    # 학습 하이퍼파라미터 (요청에 따라 기본값 수정)
    parser.add_argument("--batch_size", type=int, default=32, help="배치 사이즈")
    parser.add_argument("--image_size", type=int, default=256, help="입력 이미지 크기")
    parser.add_argument("--accumulate_grad_batches", type=int, default=1, help="그래디언트 누적 스텝. VRAM 부족 시 사용 (예: 2)")
    parser.add_argument("--epochs", type=int, default=50, help="총 학습 에포크")
    parser.add_argument("--lr", type=float, default=1e-4, help="최대 학습률")
    parser.add_argument("--unfreeze_epoch", type=int, default=5, help="백본을 동결 해제할 에포크")
    parser.add_argument("--warmup_epochs", type=int, default=3, help="웜업 에포크 수")
    parser.add_argument("--weight_decay", type=float, default=1e-5, help="Weight decay 값")

    # 데이터 증강 옵션
    parser.add_argument("--use_cutmix", action='store_true', help="CutMix 사용 여부")
    parser.add_argument("--use_mixup", action='store_true', help="MixUp 사용 여부")

    args = parser.parse_args([])  # Pass an empty list to avoid parsing Colab kernel arguments
    main(args)