# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RlL8pAima9BG5bvjhSmHCHsJLQ2LH8IZ
"""

!pip install pytorch-lightning
!pip install -U torchvision

import os
import argparse
import torch
import pytorch_lightning as pl
from torch import nn
from torch.nn import functional as F
from torch.utils.data import DataLoader, default_collate
from torchvision.datasets import ImageFolder
from torchvision import __version__ as torchvision_version
from torchvision.transforms import v2 as transforms
from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint

from google.colab import drive
drive.mount('/content/drive')

import math
from torch.optim.lr_scheduler import LambdaLR
from torchmetrics.classification import MulticlassF1Score, MulticlassPrecision, MulticlassRecall
import torchvision.models as models
import torch.nn as nn # Import torch.nn

try:
    # torchvision v0.17+ (or where SoftTargetCrossEntropyLoss exists)
    from torchvision.v2.loss import CrossEntropyLoss as SoftTargetCrossEntropyLoss
except ImportError:
    # ì´ì „ ë²„ì „ í˜¸í™˜ ë˜ëŠ” ê¸°ë³¸ PyTorch CrossEntropyLoss
    # SoftTargetCrossEntropyLossê°€ torchvisionì— ì—†ëŠ” ê²½ìš° torch.nn.CrossEntropyLoss ì‚¬ìš©
    from torch.nn import CrossEntropyLoss as SoftTargetCrossEntropyLoss


def get_cosine_schedule_with_warmup(optimizer, warmup_epochs, total_epochs):
    def lr_lambda(current_epoch):
        if current_epoch < warmup_epochs:
            return float(current_epoch) / float(max(1, warmup_epochs))
        progress = float(current_epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))
        return 0.5 * (1.0 + math.cos(math.pi * progress))
    return LambdaLR(optimizer, lr_lambda)

class ProductionPlantClassifier(pl.LightningModule):
    """
    ëª¨ë“  í”¼ë“œë°±ì´ ë°˜ì˜ëœ ìµœì¢… ë²„ì „ì˜ ë¶„ë¥˜ ëª¨ë¸ì…ë‹ˆë‹¤.
    """
    def __init__(
        self,
        num_classes,
        total_epochs,
        learning_rate=1e-4,
        unfreeze_start_epoch=5,
        warmup_epochs=3,
        dropout_rate=0.2,
        weight_decay=1e-5,
    ):
        super().__init__()
        self.save_hyperparameters()
        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
        for name, param in self.resnet.named_parameters():
            if not name.startswith(("layer4", "fc")):
                param.requires_grad = False
        in_features = self.resnet.fc.in_features
        self.resnet.fc = nn.Sequential(nn.Dropout(p=self.hparams.dropout_rate), nn.Linear(in_features, num_classes))
        # Use the imported SoftTargetCrossEntropyLoss (which will be torch.nn.CrossEntropyLoss if torchvision.v2 import fails)
        self.soft_loss_fn = SoftTargetCrossEntropyLoss()
        self.metrics = nn.ModuleDict({
            'f1': MulticlassF1Score(num_classes=num_classes, average='macro'),
            'precision': MulticlassPrecision(num_classes=num_classes, average='macro'),
            'recall': MulticlassRecall(num_classes=num_classes, average='macro')
        })

    def _get_loss(self, y_hat, y):
        # ë¼ë²¨ íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ ì†ì‹¤ í•¨ìˆ˜ ì ìš©
        if y.ndim > 1 and y.shape[1] == self.hparams.num_classes:
            # Check if SoftTargetCrossEntropyLoss is available, otherwise use standard CrossEntropyLoss
            if isinstance(self.soft_loss_fn, nn.CrossEntropyLoss):
                 # For standard CrossEntropyLoss with soft labels, use F.cross_entropy
                 return F.cross_entropy(y_hat, y)
            else:
                # Use SoftTargetCrossEntropyLoss if successfully imported from torchvision.v2
                return self.soft_loss_fn(y_hat, y) # for CutMix/MixUp soft labels
        else:
            return F.cross_entropy(y_hat, y) # for standard hard labels

    def on_train_epoch_start(self):
        # íŠ¹ì • ì—í¬í¬ì— ë°±ë³¸ ë™ê²° í•´ì œ ë° ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ì´ˆê¸°í™”
        if self.current_epoch == self.hparams.unfreeze_start_epoch:
            print(f"\n[Epoch {self.current_epoch}] Unfreezing backbone and resetting optimizer state.")
            for p in self.resnet.parameters():
                if not p.requires_grad:
                    p.requires_grad = True
            opt = self.trainer.optimizers[0]
            fc_ids = {id(p) for p in self.resnet.fc.parameters()}
            for p, state in list(opt.state.items()):
                if id(p) not in fc_ids:
                    opt.state.pop(p)

    def forward(self, x):
        return self.resnet(x)

    def _calculate_and_log_metrics(self, step_name, y_hat, y, on_step=False, on_epoch=True, prog_bar=False):
        preds = y_hat.argmax(dim=1)
        # Ensure targets are correctly formatted (long type for CrossEntropyLoss and torchmetrics)
        targets = y.argmax(dim=1).long() if y.ndim > 1 else y.long()
        acc = (preds == targets).float().mean()
        self.log(f"{step_name}_acc", acc, on_step=on_step, on_epoch=on_epoch, prog_bar=True)
        metric_results = {name: metric(preds, targets) for name, metric in self.metrics.items()}
        self.log_dict({f"{step_name}_{k}": v for k, v in metric_results.items()}, on_step=on_step, on_epoch=on_epoch, prog_bar=prog_bar)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self._get_loss(y_hat, y)
        self.log("train_loss", loss, on_step=True, on_epoch=True)
        self._calculate_and_log_metrics("train", y_hat, y, on_step=True, on_epoch=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self._get_loss(y_hat, y)
        self.log("val_loss", loss, on_step=False, on_epoch=True, prog_bar=True)
        self._calculate_and_log_metrics("val", y_hat, y, on_step=False, on_epoch=True, prog_bar=True)

    def test_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self._get_loss(y_hat, y)
        self.log("test_loss", loss, on_step=False, on_epoch=True)
        self._calculate_and_log_metrics("test", y_hat, y, on_step=False, on_epoch=True)

    def configure_optimizers(self):
        # BN, Bias íŒŒë¼ë¯¸í„°ëŠ” weight decayì—ì„œ ì œì™¸
        no_decay = []
        decay = []
        for name, param in self.named_parameters():
            if not param.requires_grad: continue
            if param.ndim <= 1 or name.endswith(".bias"):
                no_decay.append(param)
            else:
                decay.append(param)

        optimizer_groups = [
            {'params': decay, 'weight_decay': self.hparams.weight_decay},
            {'params': no_decay, 'weight_decay': 0.0}
        ]

        optimizer = torch.optim.AdamW(optimizer_groups, lr=self.hparams.learning_rate)

        scheduler = get_cosine_schedule_with_warmup(
            optimizer,
            warmup_epochs=self.hparams.warmup_epochs,
            total_epochs=self.hparams.total_epochs
        )
        return {"optimizer": optimizer, "lr_scheduler": {"scheduler": scheduler, "interval": "epoch"}}

class PlantDataModule(pl.LightningDataModule):
    def __init__(self, data_dir: str, batch_size: int, image_size: int, use_cutmix: bool, use_mixup: bool, num_workers: int):
        super().__init__()
        self.save_hyperparameters()
        self.mean, self.std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]
        self.num_classes_val = None
        self.collator = None

    def setup(self, stage: str = None):
        if self.num_classes_val is None:
            train_path = os.path.join(self.hparams.data_dir, "train")
            self.num_classes_val = len([d for d in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, d))])

        aug_list = []
        if self.hparams.use_cutmix and self.hparams.use_mixup:
            raise ValueError("CutMixì™€ MixUpì„ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        if self.hparams.use_cutmix: aug_list.append(transforms.CutMix(num_classes=self.num_classes_val, alpha=1.0))
        if self.hparams.use_mixup: aug_list.append(transforms.MixUp(num_classes=self.num_classes_val, alpha=1.0))
        self.collator = aug_list[0] if aug_list else None

        self.train_transform = transforms.Compose([
            transforms.RandomResizedCrop(self.hparams.image_size, scale=(0.8, 1.0)),
            transforms.RandomHorizontalFlip(),
            transforms.TrivialAugmentWide(),
            transforms.ToTensor(),
            transforms.Normalize(mean=self.mean, std=self.std),
        ])

        self.val_test_transform = transforms.Compose([
            transforms.Resize(self.hparams.image_size + 32),
            transforms.CenterCrop(self.hparams.image_size),
            transforms.ToTensor(),
            transforms.Normalize(mean=self.mean, std=self.std),
        ])

        if stage == "fit" or stage is None:
            self.train_dataset = ImageFolder(root=os.path.join(self.hparams.data_dir, "train"), transform=self.train_transform)
            self.val_dataset = ImageFolder(root=os.path.join(self.hparams.data_dir, "val"), transform=self.val_test_transform)
        if stage == "test" or stage is None:
            self.test_dataset = ImageFolder(root=os.path.join(self.hparams.data_dir, "test"), transform=self.val_test_transform)

    def _collate_fn(self, batch):
        imgs, labels = default_collate(batch)
        assert imgs.ndim == 4, f"ì´ë¯¸ì§€ í…ì„œëŠ” 4ì°¨ì›ì´ì–´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬: {imgs.ndim}D"
        if self.collator:
            return self.collator(imgs, labels)
        return imgs, labels

    def train_dataloader(self):
        return DataLoader(
            self.train_dataset, batch_size=self.hparams.batch_size, shuffle=True,
            num_workers=self.hparams.num_workers, pin_memory=True,
            persistent_workers=self.hparams.num_workers > 0, collate_fn=self._collate_fn
        )

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,
                          persistent_workers=self.hparams.num_workers > 0, pin_memory=True)

    def test_dataloader(self):
        return DataLoader(self.test_dataset, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,
                          persistent_workers=self.hparams.num_workers > 0, pin_memory=True)

def main(args):
    assert float(torchvision_version.split('.')[1]) >= 17, "ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” TorchVision 0.17.0 ì´ìƒì´ í•„ìš”í•©ë‹ˆë‹¤."
    torch.backends.cudnn.benchmark = True
    pl.seed_everything(42, workers=True)

    datamodule = PlantDataModule(
        data_dir=args.data_dir,
        batch_size=args.batch_size,
        image_size=args.image_size,
        use_cutmix=args.use_cutmix,
        use_mixup=args.use_mixup,
        num_workers=args.num_workers
    )

    datamodule.setup("fit")

    model = ProductionPlantClassifier(
        num_classes=datamodule.num_classes_val,
        total_epochs=args.epochs,
        learning_rate=args.lr,
        unfreeze_start_epoch=args.unfreeze_epoch,
        warmup_epochs=args.warmup_epochs,
        weight_decay=args.weight_decay
    )

    lr_monitor = LearningRateMonitor(logging_interval='epoch')
    checkpoint_callback = ModelCheckpoint(
        monitor='val_f1', mode='max',
        dirpath=args.checkpoint_dir,
        filename='plant-model-{epoch:02d}-{val_f1:.3f}',
        save_top_k=1, verbose=True
    )

    trainer = pl.Trainer(
        max_epochs=args.epochs,
        accelerator='gpu', devices=args.gpus,
        precision="16-mixed",
        gradient_clip_val=1.0,
        accumulate_grad_batches=args.accumulate_grad_batches,
        callbacks=[lr_monitor, checkpoint_callback],
        log_every_n_steps=10
    )

    print("ğŸš€ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    trainer.fit(model, datamodule=datamodule)

    print("âœ… í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìµœì ì˜ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤...")
    trainer.test(datamodule=datamodule, ckpt_path='best')

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="PyTorch Lightning Plant Disease Classifier Training")

    # ê²½ë¡œ ë° ê¸°ë³¸ ì„¤ì •
    parser.add_argument("--data_dir", type=str, default= r"F:\images", help="ë°ì´í„°ì…‹ ê²½ë¡œ")
    parser.add_argument("--checkpoint_dir", type=str, default=r"F:\checkpoints", help="ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ")
    parser.add_argument("--gpus", type=int, default=1, help="ì‚¬ìš©í•  GPU ìˆ˜")
    # num_workersì˜ ê¸°ë³¸ê°’ì„ 0ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ë©€í‹°í”„ë¡œì„¸ì‹± ê´€ë ¨ ë¬¸ì œë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
    parser.add_argument("--num_workers", type=int, default=0, help="ë°ì´í„° ë¡œë” ì›Œì»¤ ìˆ˜")

    # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ìš”ì²­ì— ë”°ë¼ ê¸°ë³¸ê°’ ìˆ˜ì •)
    parser.add_argument("--batch_size", type=int, default=32, help="ë°°ì¹˜ ì‚¬ì´ì¦ˆ")
    parser.add_argument("--image_size", type=int, default=256, help="ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°")
    parser.add_argument("--accumulate_grad_batches", type=int, default=1, help="ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…. VRAM ë¶€ì¡± ì‹œ ì‚¬ìš© (ì˜ˆ: 2)")
    parser.add_argument("--epochs", type=int, default=50, help="ì´ í•™ìŠµ ì—í¬í¬")
    parser.add_argument("--lr", type=float, default=1e-4, help="ìµœëŒ€ í•™ìŠµë¥ ")
    parser.add_argument("--unfreeze_epoch", type=int, default=5, help="ë°±ë³¸ì„ ë™ê²° í•´ì œí•  ì—í¬í¬")
    parser.add_argument("--warmup_epochs", type=int, default=3, help="ì›œì—… ì—í¬í¬ ìˆ˜")
    parser.add_argument("--weight_decay", type=float, default=1e-5, help="Weight decay ê°’")

    # ë°ì´í„° ì¦ê°• ì˜µì…˜
    parser.add_argument("--use_cutmix", action='store_true', help="CutMix ì‚¬ìš© ì—¬ë¶€")
    parser.add_argument("--use_mixup", action='store_true', help="MixUp ì‚¬ìš© ì—¬ë¶€")

    args = parser.parse_args([])  # Pass an empty list to avoid parsing Colab kernel arguments
    main(args)